---
layout: page
title: Mining Alice's Wonderland
permalink: /mining-alices-wonderland/
---
[caption id="attachment_413" align="alignright" width="376"]![alice catterpillar](https://markzwart.files.wordpress.com/2017/07/alice-catterpillar.jpg) 'Explain yourself!'[/caption] As a kid I was captivated by the strange world of Disney's Alice in Wonderland: nothing seemed to make sense and everything was wonderfully weird and exciting. When I read the 'real' book as an adult, I found out what also gave it's appeal to a kids mind: the strange context with questions being asked in Alice will make you [wonder off...](https://www.youtube.com/watch?v=bZl7zl2Yozk)
I decided it was time to learn some text mining and learned about the **[gutenbergr](https://cran.r-project.org/web/packages/gutenbergr/vignettes/intro.html) **library. This  library allows you to scrape information about writers, books and even the books itself from the [project Gutenberg](https://www.gutenberg.org/). And since Alice was on there, it was a no-brainer to have [Alice's Adventures in Wonderland](https://www.gutenberg.org/files/11/11-h/11-h.htm) as a try-out.  What I wanted to know: how are the characters in Alice's world viewed? In text mining terms: what are the sentiments associated with the characters?
For this tutorial I've assumed that you're pretty familiar with the **tidyverse** and **ggplot2**. First I'll discuss the concepts that drove the script, after which I'll jump into the technical workout of these concepts. The final script can be found in a link on the end of the tutorial.

# The building blocks

## Sentiments

Two of the most important datasets that are part of the **[tidytext](https://cran.r-project.org/web/packages/tidytext/vignettes/tidytext.html)** package are the _stop_words_ and _sentiments_. The first is useful when removing common words like, and, or or the. The other, _sentiments_ is useful for us to determine the emotions associated with the characters. The sentiment dataset contains 4 columns:

*   lexicon - the dataset contains several lexicons, each having their own characteristic way of  representing sentiments:
    *   [nrc](http://www.nrc-cnrc.gc.ca/eng/rd/ict/emotion_lexicons.html) - This is the lexicon we'll be using. It associates words with 8 basic emotions. These sentiments are found in the _sentiment_ column.
    *   bing - This lexicon divides words in either having negative or positive connotations. Which of it is can be found in the _sentiment_ column.
    *   AFINN - This lexicon also divides words in negative or positive connotations, but takes a scale approach inste ad of the bing binary approach.
*   word - you use this column to join the dataset on
*   sentiment - Depending on the lexicon you're using this is filled with descriptions.
*   score - if you use the AFINN lexicon this column contains the numerical value.

The nrc dataset I'll be using attributes one or more sentiments per word, which makes sense: words can evoke several emotions depending on context. I assume the 'right' sentiments will bubble up by the surrounding word sentiments.

## Characters

[caption id="attachment_518" align="alignright" width="238"]![tools](https://markzwart.files.wordpress.com/2017/07/tools.jpg) Off with their heads![/caption] This one speaks for itself. A vector of characters is what was needed, and I saw little options that just type all the characters myself. Typing stuff like that makes me understand the Queen of Hearts....

## Paragraphs

The paragraphs are the thing that tie the sentiments to the characters. I assumed that the sentiments that co-appear in the paragraph with characters, tells us something about the character. The sentiment profile then, is partly determined by the relative frequencies in which the sentiments co-appear with a character in paragraphs. Relative frequencies are calculated like this:

$latex \displaystyle f_{sentiment} = \frac{n_{sentiment}}{n_{paragraph}}$

Relative frequencies in itself is not good enough because the book is probably scewed in a certain sentimental direction, which does not help if we want to know what makes a character unique. To counter this I use something I called lift: the relative sentiment frequency of corrected by the relative frequency of the total. As the total I've taken Alice's relative sentiment frequencies. So the lift was calculated like this:

$latex \displaystyle lift_{sentiment} = \frac{f_{character}}{f_{alice}}$

Luckily the ebook had white lines to delimit the paragraphs;  sometimes life is easy.

## Plutchik's wheel

[caption id="attachment_472" align="alignright" width="367"]![715px-Plutchik-wheel.svg](https://markzwart.files.wordpress.com/2017/07/715px-plutchik-wheel-svg-e1500667984557.png) Plutchik's wheel of emotions[/caption] Remember the lexicons? I don't know whether this is coicidence, but the sentiment terms used in the nrc lexicon fit surprisingly well with [Plutchik's wheel of emotions](https://en.wikipedia.org/wiki/Contrasting_and_categorization_of_emotions#Plutchik.27s_wheel_of_emotions). As it is such a nice fit, I thought I'd use it as the basis of my plots. As a psychologist, my first thought was: spider-graphs; create a character profile by plotting the value sentiment confidence of a character on each of the emotion scales and connect the dots. To do this I put the confidence value on the x axis, and rotated it to the corresponding emotion petal to find out the x and y coordinates of that sentiment value. For example If you've seen spider-graphs a few times you know they can be very confusing when comparing multiple profiles: after more then 3 profiles spider-graphs become virtually unreadable. So I'd thought I'd dumb them down a bit by only displaying the center of gravity of each spider graph.

# The script

## Drawing Plutchik's wheel

For this I'll be using the **ggplot2** library's _[geom_polygon](http://ggplot2.tidyverse.org/reference/geom_polygon.html) _layer. With ggplot2 you can combine seperate ggplots by adding them. Thinking I might need the Plutchik plot again I put it in a  function that does the drawing; the function takes the wheels radius as a parameter. I made the wheel's radius variable to ensure the characters plotted in the wheel are optimally displayed. [code gutter="false" language="r"]plutchik_wheel <- function(radius)[/code] A vector is created with the hex colors codes that are used to color the Plutchik's wheel. [code gutter="false" language="r"]plutchik_colors <- c( "#8cff8c", "#ffffb1", "#ffc48c", "#ff8c8c", "#ffc6ff","#8c8cff", "#a5dbff", "#8cc68c")[/code] Plutchik's wheel has 8 spokes, which I'll refer to as petals. To create a closed polygon for each petal, 5 points draw a petal: one from the origin (0, 0), three points on the perimeter of the wheel, and one again at the origin to close the polygon. The points at the perimeter of the wheel are part of the total wheel. So calculate these points we take the radius of the circle, and rotate them by 22.5 degrees (360 degreed / (8 petals * petal edges)). First we create a vector with the rotations needed for one petal: [code gutter="false" language="r"]petal <- rep(c(0, 1:3, 0) ,8)[/code] This would repeat the same petal over and over again on the same place, but instead we want the petals drawn next to each other. For this we'll transpose the petal vector so that each petal will have its own unique position: [code gutter="false" language="r"]petal_transpose <- rep(c(0, 2, 2, 2, 0), 8) * rep(c(0:7), each =5) petal_transposed <- (petal + petal_transpose)[/code] Now the transposed petals are converted to radians to calculate the petal coordinates at the next step. [code gutter="false" language="r"]radian_petals <- petal_transposed * 22.5 * pi/180[/code] Now we create two vectors for the coordinates; _x_ and _y_: [code gutter="false" language="r"]x <- radius * (petal > 0) * cos(radian_petals) y <- radius * sin(radian_petals) [/code] Then we create a vector for grouping the other vectors by petal, with this we'll be able to color the individual petals. [code gutter="false" language="r"]id <- as.factor(rep(c(1:8), each = 5))[/code] Now we've got all ingredients for drawing the petals. Next a factored vector is created with the petal names, with the names positions so the show in the middle of the perimeter. All other positions in the vector have NA as a value so no text will be displayed, this will generate a warning message that 32 values are removed that conating missing values (geom_text). [code gutter="false" language="r"]emotions <- factor(c( NA, NA, "Trust", NA, NA, NA, NA, "Joy", NA, NA, NA, NA, "Anticipation", NA, NA, NA, NA, "Anger", NA, NA, NA, NA, "Disgust", NA, NA, NA, NA, "Sadness", NA, NA, NA, NA, "Surprise", NA, NA, NA, NA, "Fear", NA, NA ), levels = c("Trust", "Joy", "Anticipation", "Anger", "Disgust", "Sadness", "Surprise","Fear"), ordered = TRUE)[/code] All the vectors are combined in one data frame that can be used in a ggplot. [code gutter="false" language="r"]tbl_plutchik_wheel <- data.frame(id, x, y, emotions)[/code] Lastly a ggplot is created, which can later be used as part of the final plot stack. The custom ggplot theme removes all unnecesary clutter (grids, ticks and titles of the axes and the legend of the colors in the wheel). [code gutter="false" language="r"]wheel <- ggplot() + geom_polygon(data = tbl_plutchik_wheel, aes(x, y, fill=id, group=id)) + geom_text(data = tbl_plutchik_wheel, aes(x, y, label = emotions)) + scale_fill_manual(values = plutchik_colors) + theme( axis.line = element_blank(), axis.text.x = element_blank(), axis.text.y = element_blank(), axis.ticks = element_blank(), axis.title.x = element_blank(), axis.title.y = element_blank(), legend.position = "none", panel.background = element_rect(fill = "white") ) [/code]

## Getting the book

To retrieve Alice's Adventures in Wonderland we first need to load the **gutenbergr** library. [code gutter="false" language="r"]library(gutenbergr)[/code] You can retreive the book by using the _gutenberg_download _function and passing the book ID as a parameter. The ID of [Alice's Adventures in Wonderland](https://www.gutenberg.org/ebooks/11) is 11\. The ID of the ebook is the last number of the URL when browsing for the book on the Project Gutenberg website. [code gutter="false" language="r"]book_alice <- gutenberg_download(11)[/code]

## Detecting the paragraphs

Besides using the **tidyverse** library for this tutorial, we're going to use two additional libraries:

*   **[tidytext](https://cran.r-project.org/web/packages/tidytext/vignettes/tidytext.html)** - this library brings the tidyverse and text mining together. It allows you to make tidy data-frames to pass on to text mining tools.
*   [**stringr**](https://cran.r-project.org/web/packages/stringr/vignettes/stringr.html) - a library packed with functions for text manipulation.

So let's start by loading the necesarry libraries: [code gutter="false" language="r"] library(tidyverse) library(tidytext) library(stringr) [/code] In the next section we take the downloaded book and first add some line numbers to it; which we use to skip some irrelevant lines of the book containing the title and such. After which we find out the rows that are just chapter headings, and the count the words in each line. We mark the lines which doesn't contain a chapter header and which have words as paragraphs. [code gutter="false" language="r"]tbl_paragraphs >- book_alice %>% mutate(line = row_number()) %>% filter(line >= 10 & line < 3338) %>% # Skipping irrelevant lines mutate(is_chapter_title = str_detect(text, "CHAPTER")) %>% # Detect chapter titles mutate(qty_words = sapply(gregexpr("[[:alpha:]]+", text), function(x) sum(x >; 0))) %>% # Count words in a line mutate(is_paragraph = !is_chapter_title & qty_words > 0) # Mark lines that are not chapter titles and contain words as part of a sections[/code] After this the [_rle_](https://stat.ethz.ch/R-manual/R-devel/library/base/html/rle.html) function is used to find consequtive rows belong to a paragraph and when it is broken by non-paragraph lines. Each row in resulting data-frame tells us how many lines are part of one paragraph, or the number of lines between paragraphs. [code gutter="false" language="r"]tbl_paragraph_id <- data.frame(length = rle(tbl_paragraphs$is_paragraph)[[1]], value = rle(tbl_paragraphs$is_paragraph)[[2]])[/code] The we use this data frame to create a new column in the paragraph data frame to set an identifier for each paragraph. With the function _seq_ a number is generated for each set of consequtive lines. The _rep_ function repeats this number for the number of consequtive lines. In the next _mutate_ function the paragraph identifiers are removed if the set of consequtive rows are not part of a paragraph. [code gutter="false" language="r"]tbl_paragraphs %<>% mutate(id_paragraph = rep(seq(1,nrow(tbl_paragraph_id),1), tbl_paragraph_id$length)) %>% mutate(id_paragraph = ifelse(is_paragraph, id_paragraph, NA))[/code] Next we'll un-nest all words in each line so each word will become one row and put it in the _tbl_words_ data frame. We use the function _anti_join_ to remove any of the stopwords. Then only "words" are kept that only consist of letters. After this we only keep the words, the paragraph identifier in which they appear, and the frequency of their usage in that paragraph. [code gutter="false" language="r"]tbl_word <- tbl_paragraphs %>% unnest_tokens(word, text) %>% # Make a new row for each words encountered in the text anti_join(stop_words, by = "word") %>% # Remove stop words mutate(word = str_extract(word, "[a-z']+")) %>% # Get only words that consists only of characters group_by(id_paragraph, word) %>% summarise(qty_word = n()) # Count the frequency of words[/code]

## Finding sentiments

We'll use the sentiments data frame from the **tidytext** package to create a custom data frame. But before we do that we'll create our own data frame that specifies how many degrees each sentiment should be rotated to fit it on Plutchik's wheel. The sentiments are then factored ordered by their order in Plutchik's wheel. [code gutter="false" language="r"]sentiment_order <- c("fear", "trust", "joy", "anticipation", "anger", "disgust", "sadness", "surprise") degrees_sentiment <- c(0, 45, 90, 135, 180, 225, 270, 315) tbl_sentiments <- data.frame(sentiment_order, degrees_sentiment) tbl_sentiments %<>% rename(sentiment = sentiment_order) %>% mutate(sentiment = factor(sentiment, levels = sentiment_order, ordered = TRUE)) rm(degrees_sentiment)[/code] The custom data frame, _tbl_sentiments_, that we'll create here just uses the nrc lexicon, and leaves out the 'positive' and 'negative' sentiments; which don't make much sense in the wheel. We'll just take the columns of that data frame which are useful to our analysis: words and their sentiments. The sentiments, which are factored variables in the original dataset, are refactored using the order in which they appear in Plutchik's wheel; before they can be refactored they have to be converted to strings to remove previous factoring. [code gutter="false" language="r"]tbl_sentiment_lexicon <- sentiments %>% filter(sentiment != "negative" & sentiment != "positive") %>% filter(lexicon == "nrc") %>% select(word, sentiment) %>% mutate(sentiment = as.character(sentiment)) %>% mutate(sentiment = factor(sentiment, levels = sentiment_order, ordered = TRUE))[/code] This pepared _tbl_sentiment_lexicon_ data frame is used together with the _tbl_word_ data frame to tie the sentiments to paragraphs. The sentiments are grouped by paragraph and their frequency is counted. [code gutter="false" language="r"]tbl_par_sentiments <- tbl_words %>% inner_join(tbl_sentiment_lexicon, by = "word") %>% group_by(id_paragraph, sentiment) %>% summarise(qty_sentiment = n())[/code] When we look at the occurence of sentiments throughout the book, we see that surprise and fear make place for trust. This fits with the idea that Alice will get more used to the absurdities of Wonderland the longer she stays there. [caption id="attachment_545" align="alignnone" width="1078"]![sentiment_density](https://markzwart.files.wordpress.com/2017/07/sentiment_density.png) Occurrence of sentiments throughout the book[/caption]

## Finding characters

To find the appearance of the characters in paragraphs the manually filled vector _personea_ is used with the _tbl_word_ data frame. Only the words that match the personea are kept. These are then grouped by paragraph, and their occurence in the paragraphs is counted. [code gutter="false" language="r"]tbl_par_personea <- tbl_words %>% mutate(is_person = word %in% personea) %>% # Mark words that are characters filter(is_person) %>% select(id_paragraph, persona = word, qty_word) %>% group_by(id_paragraph, persona) %>% summarise(qty_mentions = sum(qty_word))[/code] In the plot below, you can see how Alice, unsurprisingly, plays a big role throughout the book. [caption id="attachment_526" align="alignnone" width="888"]![person_appearance](https://markzwart.files.wordpress.com/2017/07/person_appearance.png) The occurence of the characters throughout Alice's Adventures in Wonderland.[/caption]

## Combining persons and sentiments

Before two data frames we just created, tbl_par_personea and _tbl_par_sentiments_, are joined to find out which sentiments are associated with characters, the total number of mentions of a character in the book are counted without creating an intermediary table. This is achieved by using the _mutate_ function instead of the usual _summarise_ function after the _group_by_ function. After the sentiments are matched with the persons by paragraphs, the [code gutter="false" language="r"]tbl_persona_sentiments <- tbl_par_personea %>% group_by(persona) %>% mutate(qty_paragraphs = n_distinct(id_paragraph)) %>% inner_join(tbl_par_sentiments, by ="id_paragraph") %>% # Join sentiments with persons group_by(persona, sentiment, qty_paragraphs) %>% summarise(qty_sentiments = sum(qty_sentiment)) %>% ungroup() %>% group_by(persona) %>% mutate(qty_sentiment_persona = sum(qty_sentiments)) %>% ungroup() %>% mutate(perc_sentiments = qty_sentiments/qty_sentiment_persona) %>% complete(persona, nesting(sentiment), fill = list(qty_sentiments = 0, qty_paragraphs = 0, qty_sentiment_persona = 0, perc_sentiments = 0))[/code] The characters that appear very few times in the book run the risk of having sentiment profiles that are out of whack. They are filtered out: [code gutter="false" language="r"]tbl_persona_significant <- tbl_persona_sentiments %>% group_by(persona) %>% summarise(qty_sentiments = sum(qty_sentiments)) %>% filter(qty_sentiments > 35) tbl_persona_sentiments %<>% filter(persona %in% tbl_persona_significant$persona)[/code]

## A character's sentiment profile

[code gutter="false" language="r"]tbl_alice_sentiments <- tbl_persona_sentiments %>% filter(persona == "alice") %>% select(sentiment, perc_alice = perc_sentiments) tbl_persona_sentiments %<>% inner_join(tbl_alice_sentiments, by = "sentiment") %>% mutate(lift_sentiment = perc_sentiments / perc_alice) %>% mutate(impact = abs(lift_sentiment - 1))[/code] ![person_sentiment_1](https://markzwart.files.wordpress.com/2017/07/person_sentiment_1.png)

## Building the graph

ggrepel Stacking plots

# The Result

![Alice in Wonderland](https://markzwart.files.wordpress.com/2017/07/alice-in-wonderland.png)